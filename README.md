# Building the Architecture of a General Pre-trained Transformer

This personal project focuses on developing a general pre-trained transformer model, inspired by Elliot Arledge's FreeCodeCamp tutorial on YouTube. It involves rigorous data handling, where large datasets are meticulously processed and structured for training. The project delves deep into the mathematical foundations of transformers, exploring concepts like attention mechanisms and positional encoding. Building the model from scratch, it covers every step from initializing parameters to training and fine-tuning, offering a comprehensive understanding of transformer architecture and its applications in natural language processing.

## ðŸš€ About Me
Passionate self-taught artificial intelligence developer (AI) and machine learning engineer (ML) with a formal background in technical support, network security, system administration.

## Socials
Twitter / X - https://twitter.com/TMobley96

My YouTube Channel - https://www.youtube.com/@PapaAI334

Running LLMs on Your Local Machine - https://tjmobley.hashnode.dev/taking-control-of-ai-running-llms-on-your-local-machine

Linkedin - https://www.linkedin.com/in/1tmobley/

Hugging Face - https://huggingface.co/tmobley96

## Research Papers Used:
[Attention is All You Need](https://arxiv.org/pdf/1706.03762.pdf)

[A Survey of LLMs](https://arxiv.org/pdf/2303.18223.pdf)

[QLoRA: Efficient Finetuning of Quantized LLMs](https://arxiv.org/pdf/2305.14314.pdf)

[Elliot's tutorial on Youtube](https://youtu.be/UU1WVnMk4E8?si=wigLBo7kr2nvUz-O)
